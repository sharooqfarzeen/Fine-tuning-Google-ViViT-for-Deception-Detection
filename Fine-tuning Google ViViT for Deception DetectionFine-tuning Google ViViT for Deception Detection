{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10157792,"sourceType":"datasetVersion","datasetId":6271859}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sharooqfarzeenak/fine-tuning-google-vivit-for-deception-detection?scriptVersionId=212319307\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Fine-tuning Google Video Vision Transformer (ViViT) using Real-life Deception Detection Dataset\n\n## Use-cases\n\n1. Job Interviews\n2. Criminal Proceedings, Trials\n\n## About\n\nDataset used - [Real-life Deception Detection Dataset](https://public.websites.umich.edu/~zmohamed/resources.html)\n\nModel Used - [ViViT (Video Vision Transformer) - Video Classifier](https://huggingface.co/google/vivit-b-16x2)\n\nResearch Paper for the dataset - [Deception Detection using Real-life Trial Data](https://web.eecs.umich.edu/~zmohamed/PDFs/Trial.ICMI.pdf)","metadata":{}},{"cell_type":"markdown","source":"# Installing required modules","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers torch scikit-learn pyav datasets tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:13:52.201608Z","iopub.execute_input":"2024-12-10T14:13:52.201885Z","iopub.status.idle":"2024-12-10T14:14:03.317521Z","shell.execute_reply.started":"2024-12-10T14:13:52.201856Z","shell.execute_reply":"2024-12-10T14:14:03.316412Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Setting device to GPU, if available\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:03.319568Z","iopub.execute_input":"2024-12-10T14:14:03.319867Z","iopub.status.idle":"2024-12-10T14:14:03.327735Z","shell.execute_reply.started":"2024-12-10T14:14:03.319836Z","shell.execute_reply":"2024-12-10T14:14:03.326854Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Importing required modules\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport av # Video processing\n\nfrom datasets import Dataset, load_from_disk\n\nimport torch\nfrom transformers import TrainingArguments, Trainer # Fine-tuning\nfrom transformers import VivitImageProcessor, VivitForVideoClassification # Model\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:03.328986Z","iopub.execute_input":"2024-12-10T14:14:03.329305Z","iopub.status.idle":"2024-12-10T14:14:04.961289Z","shell.execute_reply.started":"2024-12-10T14:14:03.329275Z","shell.execute_reply":"2024-12-10T14:14:04.960391Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"np.random.seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:04.963758Z","iopub.execute_input":"2024-12-10T14:14:04.965069Z","iopub.status.idle":"2024-12-10T14:14:04.968953Z","shell.execute_reply.started":"2024-12-10T14:14:04.965038Z","shell.execute_reply":"2024-12-10T14:14:04.967938Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Preparing the Data","metadata":{}},{"cell_type":"markdown","source":"## Function to read the video, get selected frames and convert it to a NumPy Array","metadata":{}},{"cell_type":"code","source":"import av\n\n# Function to read the video, get selected frames and convert it to a NumPy Array\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:04.969788Z","iopub.execute_input":"2024-12-10T14:14:04.970004Z","iopub.status.idle":"2024-12-10T14:14:04.985924Z","shell.execute_reply.started":"2024-12-10T14:14:04.969981Z","shell.execute_reply":"2024-12-10T14:14:04.985153Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Function to pick frame indices from the video for training","metadata":{}},{"cell_type":"code","source":"def sample_frame_indices(no_of_frames, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        no_of_frames (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    # converted_len = int(clip_len * frame_sample_rate)\n    # end_idx = np.random.randint(converted_len, seg_len)\n    # start_idx = end_idx - converted_len\n    # indices = np.linspace(start_idx, end_idx, num=clip_len)\n    # indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    # return indices\n\n    end_idx = no_of_frames * frame_sample_rate\n    start_idx = 0\n    indices = np.linspace(start_idx, end_idx, num=no_of_frames, dtype=int)\n    return indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:04.986828Z","iopub.execute_input":"2024-12-10T14:14:04.987038Z","iopub.status.idle":"2024-12-10T14:14:04.996266Z","shell.execute_reply.started":"2024-12-10T14:14:04.987015Z","shell.execute_reply":"2024-12-10T14:14:04.995645Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Function to parse through all videos in both folders and create Dataset Dictionary","metadata":{}},{"cell_type":"code","source":"import os\n\n# Function to parse through all video in both folders and create Dataset Dictionary\ndef frames_convert_and_create_dataset_dictionary(video_dir):\n    all_videos=[]\n\n    # Creating list of all video file names\n    video_files = [\n            os.path.join(video_dir, f)\n            for f in os.listdir(video_dir)\n            if f.endswith(\".mp4\")\n    ]\n    \n    # Parsing through each file\n    for file in video_files:\n        # Extracting label name from file name\n        if \"lie\" in file.lower():\n            label = 0\n        elif \"truth\" in file.lower():\n            label = 1\n        # Initializing the container\n        container = av.open(file)\n\n        # Setting number of frames required\n        no_of_frames = 32\n        frame_sample_rate = 4\n\n        # Total frames in video\n        total_frames = container.streams.video[0].frames\n\n        # Process only if total number of frames in the video is greater than what we are seeking\n        if total_frames > (no_of_frames * frame_sample_rate):\n            indices = sample_frame_indices(no_of_frames=no_of_frames, frame_sample_rate=frame_sample_rate, seg_len=total_frames)\n            video = read_video_pyav(container=container, indices=indices)\n            all_videos.append({'video': video, 'labels': label})\n    \n    return all_videos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:04.997257Z","iopub.execute_input":"2024-12-10T14:14:04.997568Z","iopub.status.idle":"2024-12-10T14:14:05.010276Z","shell.execute_reply.started":"2024-12-10T14:14:04.997529Z","shell.execute_reply":"2024-12-10T14:14:05.009522Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Passing dataset through VivitImageProcessor","metadata":{}},{"cell_type":"code","source":"from transformers import VivitImageProcessor\n\n# Initializing image processor\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n\ndef process_example(example):\n    inputs = image_processor(list(np.array(example['video'])), return_tensors='pt')\n    inputs['labels'] = example['labels']\n    return inputs\n\ndef create_vivit_dataset(list_of_dict):\n    processed_list_of_dict = list(map(process_example,list_of_dict))\n    return processed_list_of_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:05.011102Z","iopub.execute_input":"2024-12-10T14:14:05.011359Z","iopub.status.idle":"2024-12-10T14:14:05.187036Z","shell.execute_reply.started":"2024-12-10T14:14:05.011333Z","shell.execute_reply":"2024-12-10T14:14:05.186261Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0d1e3a0c8f849af95794040753a2c46"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Preparing the training and eval datasets","metadata":{}},{"cell_type":"code","source":"# Setting training path\ntrain_videos_path = \"/kaggle/input/real-life-deception-detection-dataset/Real-life Deception Detection Dataset With Train Test/Train\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:05.187986Z","iopub.execute_input":"2024-12-10T14:14:05.188254Z","iopub.status.idle":"2024-12-10T14:14:05.191778Z","shell.execute_reply.started":"2024-12-10T14:14:05.188197Z","shell.execute_reply":"2024-12-10T14:14:05.19097Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\n# Some rows in the Dataset has frames less than 32\n# Function finds such rows and removes them\ndef remove_bad_rows(dataset):\n    bad_rows = []\n    for i,row in tqdm(enumerate(dataset)):\n     if torch.tensor(row['pixel_values']).shape[1] < 32:\n         bad_rows.append(i)\n\n    # Creating a list of indices excluding the rows to be removed\n    indices_to_keep = [i for i in range(len(dataset)) if i not in bad_rows]\n    \n    # Select only the rows with those indices\n    dataset = dataset.select(indices_to_keep)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:05.194797Z","iopub.execute_input":"2024-12-10T14:14:05.195036Z","iopub.status.idle":"2024-12-10T14:14:05.206159Z","shell.execute_reply.started":"2024-12-10T14:14:05.195012Z","shell.execute_reply":"2024-12-10T14:14:05.20528Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def pre_process(path):\n\n    # Preparing the training and eval datasets\n\n    # Converts video files to a list of dictionaries containing keys 'video' and 'labels',\n    # where 'video' contains 32 frames each from every video\n    print(\"Creating list of dictionaries...\\n\")\n    list_of_dictionaries = frames_convert_and_create_dataset_dictionary(video_dir=path)\n\n    # Passing above dictionary through VivitImageProcessor\n    print(\"Passing through VivitImageProcessor...\\n\")\n    dataset = create_vivit_dataset(list_of_dictionaries)\n\n    # Converting above dataset to Hugging Face Dataset for fine-tuning\n    print(\"Converting to Hugging Face Dataset...\\n\")\n    dataset_hf = Dataset.from_list(dataset)\n\n    # Encoding classes to the Dataset\n    print(\"Adding class encoding labels...\\n\")\n    dataset_hf = dataset_hf.class_encode_column(\"labels\")\n\n    # Removing bad rows (rows with less than 32 frames)\n    print(\"\\nFinding and removing bad rows...\\n\")\n    dataset_hf = remove_bad_rows(dataset_hf)\n\n\n    # Squeezing; fine-tuning step will throw an errror without this step\n    print(\"\\nSqueezing pixel_values...\\n\")\n    dataset_hf = dataset_hf.map(lambda x: {'pixel_values': torch.tensor(x['pixel_values']).to(device).squeeze()})\n\n    print(\"\\nSuccess.\\n\")\n\n    return dataset_hf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:05.207156Z","iopub.execute_input":"2024-12-10T14:14:05.20743Z","iopub.status.idle":"2024-12-10T14:14:05.216794Z","shell.execute_reply.started":"2024-12-10T14:14:05.207406Z","shell.execute_reply":"2024-12-10T14:14:05.215829Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Getting training dataset\ntrain_eval_dataset = pre_process(train_videos_path)\n\n# Splitting to Train and Eval sets\ntrain_eval_dataset = train_eval_dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:14:05.21775Z","iopub.execute_input":"2024-12-10T14:14:05.218084Z","iopub.status.idle":"2024-12-10T14:26:41.243306Z","shell.execute_reply.started":"2024-12-10T14:14:05.218042Z","shell.execute_reply":"2024-12-10T14:26:41.242622Z"}},"outputs":[{"name":"stdout","text":"Creating list of dictionaries...\n\nPassing through VivitImageProcessor...\n\nConverting to Hugging Face Dataset...\n\nAdding class encoding labels...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Stringifying the column:   0%|          | 0/109 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db010bfbe9cb4c619c362e2c206d40f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting to class labels:   0%|          | 0/109 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9b93f94499347198307a34dfab009ec"}},"metadata":{}},{"name":"stdout","text":"\nFinding and removing bad rows...\n\n\nSqueezing pixel_values...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/107 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85d646f722534d6c8cfd36b581893b3d"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Saving dataset to disk\ntrain_eval_dataset.save_to_disk(\"/kaggle/working/processed_datasets/train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:26:41.244483Z","iopub.execute_input":"2024-12-10T14:26:41.244745Z","iopub.status.idle":"2024-12-10T14:26:44.790459Z","shell.execute_reply.started":"2024-12-10T14:26:41.244717Z","shell.execute_reply":"2024-12-10T14:26:44.789578Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/4 shards):   0%|          | 0/96 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6403a23ae8940b28a995b7594049a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c074ba1f03c0450785ef7fcb04c68c3b"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\n\ntrain_dataset = load_from_disk(\"/kaggle/working/processed_datasets/train/train\")\neval_dataset = load_from_disk(\"/kaggle/working/processed_datasets/train/test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:30:44.505043Z","iopub.execute_input":"2024-12-10T14:30:44.505745Z","iopub.status.idle":"2024-12-10T14:30:44.518964Z","shell.execute_reply.started":"2024-12-10T14:30:44.50571Z","shell.execute_reply":"2024-12-10T14:30:44.518301Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Defining training arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:30:48.184637Z","iopub.execute_input":"2024-12-10T14:30:48.185302Z","iopub.status.idle":"2024-12-10T14:30:48.189165Z","shell.execute_reply.started":"2024-12-10T14:30:48.185268Z","shell.execute_reply":"2024-12-10T14:30:48.188269Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Defining training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=5,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    do_train=True,\n    do_eval=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:30:50.558996Z","iopub.execute_input":"2024-12-10T14:30:50.559822Z","iopub.status.idle":"2024-12-10T14:30:50.595172Z","shell.execute_reply.started":"2024-12-10T14:30:50.559785Z","shell.execute_reply":"2024-12-10T14:30:50.594524Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Defining the trainer","metadata":{}},{"cell_type":"code","source":"# Defining the trainer\nfrom transformers import VivitImageProcessor, VivitForVideoClassification\n\nmodel = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n\n\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndef compute_metrics(pred):\n    logits, labels = pred\n    logits = torch.tensor(logits)  # Convert logits to a PyTorch tensor\n    predictions = torch.argmax(logits, dim=-1)\n    acc = accuracy_score(labels, predictions)\n    return {\"accuracy\": acc}\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    processing_class=image_processor,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:31:11.821918Z","iopub.execute_input":"2024-12-10T14:31:11.822735Z","iopub.status.idle":"2024-12-10T14:31:13.539148Z","shell.execute_reply.started":"2024-12-10T14:31:11.822674Z","shell.execute_reply":"2024-12-10T14:31:13.538265Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/356M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0035979de78d4edc948351303b4c3495"}},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T14:31:26.199553Z","iopub.execute_input":"2024-12-10T14:31:26.200186Z","iopub.status.idle":"2024-12-10T15:02:38.693979Z","shell.execute_reply.started":"2024-12-10T14:31:26.200133Z","shell.execute_reply":"2024-12-10T15:02:38.693038Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241210_143136-h37fmn3h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sharooqfarzeen-other/huggingface/runs/h37fmn3h' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/sharooqfarzeen-other/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sharooqfarzeen-other/huggingface' target=\"_blank\">https://wandb.ai/sharooqfarzeen-other/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sharooqfarzeen-other/huggingface/runs/h37fmn3h' target=\"_blank\">https://wandb.ai/sharooqfarzeen-other/huggingface/runs/h37fmn3h</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [240/240 30:45, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.282400</td>\n      <td>2.027216</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.420100</td>\n      <td>1.353404</td>\n      <td>0.636364</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.911100</td>\n      <td>3.292757</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.041400</td>\n      <td>3.668586</td>\n      <td>0.636364</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.196700</td>\n      <td>3.684344</td>\n      <td>0.636364</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=240, training_loss=0.6985646343401943, metrics={'train_runtime': 1872.0254, 'train_samples_per_second': 0.256, 'train_steps_per_second': 0.128, 'total_flos': 1.23402864033792e+18, 'train_loss': 0.6985646343401943, 'epoch': 5.0})"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"## Saving the model","metadata":{}},{"cell_type":"code","source":"# Saving the model\ntrainer.save_model(\"/kaggle/working/vivit_finetuned_deception_detection\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:02:39.12452Z","iopub.execute_input":"2024-12-10T15:02:39.124872Z","iopub.status.idle":"2024-12-10T15:02:39.917876Z","shell.execute_reply.started":"2024-12-10T15:02:39.124831Z","shell.execute_reply":"2024-12-10T15:02:39.916904Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"markdown","source":"## Creating the test dataset","metadata":{}},{"cell_type":"code","source":"# Setting test video path\ntest_videos_path = \"/kaggle/input/real-life-deception-detection-dataset/Real-life Deception Detection Dataset With Train Test/Test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:02:39.919588Z","iopub.execute_input":"2024-12-10T15:02:39.919869Z","iopub.status.idle":"2024-12-10T15:02:39.924671Z","shell.execute_reply.started":"2024-12-10T15:02:39.919842Z","shell.execute_reply":"2024-12-10T15:02:39.923886Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from transformers import VivitImageProcessor\nimport av\n\ndef create_test_data(path):\n    \n    image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n    \n    test_dataset=[]\n    test_labels = []\n\n    # Creating list of all video file names\n    video_files = [\n            os.path.join(path, f)\n            for f in os.listdir(path)\n            if f.endswith(\".mp4\")\n    ] \n\n    for file in video_files:\n\n        if \"lie\" in file.lower():\n            label = 0\n        elif \"truth\" in file.lower():\n            label = 1\n            \n        container = av.open(file)\n\n        # sample 32 frames\n        indices = sample_frame_indices(clip_len=32, frame_sample_rate=2, seg_len=container.streams.video[0].frames)\n        \n        video = read_video_pyav(container=container, indices=indices)\n        \n        image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n        \n        inputs = image_processor(list(video), return_tensors=\"pt\")\n\n        test_dataset.append(inputs)\n        test_labels.append(label)\n    \n    return test_dataset, test_labels    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:02:39.925768Z","iopub.execute_input":"2024-12-10T15:02:39.926611Z","iopub.status.idle":"2024-12-10T15:02:39.940195Z","shell.execute_reply.started":"2024-12-10T15:02:39.926557Z","shell.execute_reply":"2024-12-10T15:02:39.939279Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## Predicting for test data","metadata":{}},{"cell_type":"code","source":"test_data, test_labels = create_test_data(test_videos_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:02:39.941773Z","iopub.execute_input":"2024-12-10T15:02:39.942129Z","iopub.status.idle":"2024-12-10T15:02:44.971266Z","shell.execute_reply.started":"2024-12-10T15:02:39.942088Z","shell.execute_reply":"2024-12-10T15:02:44.970365Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from transformers import VivitModel\n\ndef test(test_data):\n    \n    model = VivitForVideoClassification.from_pretrained(\"/kaggle/working/results/checkpoint-240\")\n\n    predictions = []\n    \n    for inputs in test_data:\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            logits = outputs.logits\n        \n        # model predicts one of the 400 Kinetics-400 classes\n        predicted_label = logits.argmax(-1).item()\n\n        predictions.append(predicted_label)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:02:44.972437Z","iopub.execute_input":"2024-12-10T15:02:44.973479Z","iopub.status.idle":"2024-12-10T15:02:44.978913Z","shell.execute_reply.started":"2024-12-10T15:02:44.973448Z","shell.execute_reply":"2024-12-10T15:02:44.978067Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"pred_labels = test(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:02:44.98005Z","iopub.execute_input":"2024-12-10T15:02:44.980669Z","iopub.status.idle":"2024-12-10T15:03:50.263681Z","shell.execute_reply.started":"2024-12-10T15:02:44.980639Z","shell.execute_reply":"2024-12-10T15:03:50.262737Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n\n\nscore = accuracy_score(y_true = test_labels, y_pred=pred_labels)\n\nscore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T15:03:50.264809Z","iopub.execute_input":"2024-12-10T15:03:50.265078Z","iopub.status.idle":"2024-12-10T15:03:50.27279Z","shell.execute_reply.started":"2024-12-10T15:03:50.26505Z","shell.execute_reply":"2024-12-10T15:03:50.271928Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.25"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"#### Accuracy can be improved by training with more frames per video, given better hardware capabilities","metadata":{}}]}