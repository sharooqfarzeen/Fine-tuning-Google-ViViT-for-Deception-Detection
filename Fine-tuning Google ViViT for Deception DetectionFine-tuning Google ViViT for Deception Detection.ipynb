{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10157792,"sourceType":"datasetVersion","datasetId":6271859}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sharooqfarzeenak/fine-tuning-google-vivit-for-deception-detection?scriptVersionId=212417102\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Fine-tuning Google Video Vision Transformer (ViViT) using Real-life Deception Detection Dataset\n\n## About\n\nDataset used - [Real-life Deception Detection Dataset](https://public.websites.umich.edu/~zmohamed/resources.html)\n\nModel Used - [ViViT (Video Vision Transformer) - Video Classifier](https://huggingface.co/google/vivit-b-16x2)\n\nResearch Paper for the dataset - [Deception Detection using Real-life Trial Data](https://web.eecs.umich.edu/~zmohamed/PDFs/Trial.ICMI.pdf)\n\n## Use-cases\n\n1. Assessing job interview candidates\n2. Criminal proceedings, trials","metadata":{}},{"cell_type":"code","source":"# Setting Global Variables\n\n# Number of frames to read from each video\nNO_OF_FRAMES = 32\n# One frame from every FRAME_SAMPLE_RATE number of frames will be sampled\nFRAME_SAMPLE_RATE = 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:18.515301Z","iopub.execute_input":"2024-12-11T02:06:18.515691Z","iopub.status.idle":"2024-12-11T02:06:18.521238Z","shell.execute_reply.started":"2024-12-11T02:06:18.515659Z","shell.execute_reply":"2024-12-11T02:06:18.52028Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Installing required modules","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers torch scikit-learn pyav datasets tqdm wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:18.522833Z","iopub.execute_input":"2024-12-11T02:06:18.523071Z","iopub.status.idle":"2024-12-11T02:06:27.383433Z","shell.execute_reply.started":"2024-12-11T02:06:18.523048Z","shell.execute_reply":"2024-12-11T02:06:27.382642Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Setting device to GPU, if available\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.385235Z","iopub.execute_input":"2024-12-11T02:06:27.385531Z","iopub.status.idle":"2024-12-11T02:06:27.393198Z","shell.execute_reply.started":"2024-12-11T02:06:27.385503Z","shell.execute_reply":"2024-12-11T02:06:27.392537Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# Importing required modules\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport av # Video processing\n\nfrom datasets import Dataset, load_from_disk\n\nimport torch\nfrom transformers import TrainingArguments, Trainer # Fine-tuning\nfrom transformers import VivitImageProcessor, VivitForVideoClassification # Model\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.394189Z","iopub.execute_input":"2024-12-11T02:06:27.394504Z","iopub.status.idle":"2024-12-11T02:06:27.403017Z","shell.execute_reply.started":"2024-12-11T02:06:27.394478Z","shell.execute_reply":"2024-12-11T02:06:27.402421Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"np.random.seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.405117Z","iopub.execute_input":"2024-12-11T02:06:27.405743Z","iopub.status.idle":"2024-12-11T02:06:27.418031Z","shell.execute_reply.started":"2024-12-11T02:06:27.405715Z","shell.execute_reply":"2024-12-11T02:06:27.417426Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Preparing the Data","metadata":{}},{"cell_type":"markdown","source":"## Function to read the video, get selected frames and convert it to a NumPy Array","metadata":{}},{"cell_type":"code","source":"import av\n\n# Function to read the video, get selected frames and convert it to a NumPy Array\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.418959Z","iopub.execute_input":"2024-12-11T02:06:27.419191Z","iopub.status.idle":"2024-12-11T02:06:27.430282Z","shell.execute_reply.started":"2024-12-11T02:06:27.419168Z","shell.execute_reply":"2024-12-11T02:06:27.429484Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Function to pick frame indices from the video for training","metadata":{}},{"cell_type":"code","source":"def sample_frame_indices(no_of_frames, frame_sample_rate, seg_len):\n    '''\n    Sample a given number of frame indices from the video.\n    Args:\n        no_of_frames (`int`): Total number of frames to sample.\n        frame_sample_rate (`int`): Sample every n-th frame.\n        seg_len (`int`): Maximum allowed index of sample's last frame.\n    Returns:\n        indices (`List[int]`): List of sampled frame indices\n    '''\n    # Uncomment to choose frames randomly; also comment out the next part\n    # converted_len = int(clip_len * frame_sample_rate)\n    # end_idx = np.random.randint(converted_len, seg_len)\n    # start_idx = end_idx - converted_len\n    # indices = np.linspace(start_idx, end_idx, num=clip_len)\n    # indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n    # return indices\n\n    end_idx = no_of_frames * frame_sample_rate\n    start_idx = 0\n    indices = np.linspace(start_idx, end_idx, num=no_of_frames, dtype=int)\n    return indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.431441Z","iopub.execute_input":"2024-12-11T02:06:27.432192Z","iopub.status.idle":"2024-12-11T02:06:27.448048Z","shell.execute_reply.started":"2024-12-11T02:06:27.432153Z","shell.execute_reply":"2024-12-11T02:06:27.447156Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Function to parse through all videos in both folders and create Dataset Dictionary","metadata":{}},{"cell_type":"code","source":"import os\n\n# Function to parse through all video in both folders and create Dataset Dictionary\ndef frames_convert_and_create_dataset_dictionary(video_dir):\n    all_videos=[]\n\n    # Creating list of all video file names\n    video_files = [\n            os.path.join(video_dir, f)\n            for f in os.listdir(video_dir)\n            if f.endswith(\".mp4\")\n    ]\n    \n    # Parsing through each file\n    for file in video_files:\n        # Extracting label name from file name\n        if \"lie\" in file.lower():\n            label = 0\n        elif \"truth\" in file.lower():\n            label = 1\n        # Initializing the container\n        container = av.open(file)\n\n        # # Setting number of frames required\n        # no_of_frames = 32\n        # frame_sample_rate = 4\n\n        # Total frames in video\n        total_frames = container.streams.video[0].frames\n\n        # Process only if total number of frames in the video is greater than what we are seeking\n        if total_frames > (NO_OF_FRAMES * FRAME_SAMPLE_RATE):\n            indices = sample_frame_indices(no_of_frames=NO_OF_FRAMES, frame_sample_rate=FRAME_SAMPLE_RATE, seg_len=total_frames)\n            video = read_video_pyav(container=container, indices=indices)\n            all_videos.append({'video': video, 'labels': label})\n    \n    return all_videos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.449151Z","iopub.execute_input":"2024-12-11T02:06:27.449501Z","iopub.status.idle":"2024-12-11T02:06:27.464557Z","shell.execute_reply.started":"2024-12-11T02:06:27.449461Z","shell.execute_reply":"2024-12-11T02:06:27.463769Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## Passing dataset through VivitImageProcessor","metadata":{}},{"cell_type":"code","source":"from transformers import VivitImageProcessor\n\n# Initializing image processor\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n\ndef process_example(example):\n    inputs = image_processor(list(np.array(example['video'])), return_tensors='pt')\n    inputs['labels'] = example['labels']\n    return inputs\n\ndef create_vivit_dataset(list_of_dict):\n    processed_list_of_dict = list(map(process_example,list_of_dict))\n    return processed_list_of_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.465662Z","iopub.execute_input":"2024-12-11T02:06:27.465999Z","iopub.status.idle":"2024-12-11T02:06:27.557275Z","shell.execute_reply.started":"2024-12-11T02:06:27.465963Z","shell.execute_reply":"2024-12-11T02:06:27.556259Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## Preparing the training and eval datasets","metadata":{}},{"cell_type":"code","source":"# Setting training path\ntrain_videos_path = \"/kaggle/input/real-life-deception-detection-dataset/Real-life Deception Detection Dataset With Train Test/Train\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.558572Z","iopub.execute_input":"2024-12-11T02:06:27.558813Z","iopub.status.idle":"2024-12-11T02:06:27.563539Z","shell.execute_reply.started":"2024-12-11T02:06:27.558789Z","shell.execute_reply":"2024-12-11T02:06:27.562702Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torch\n\n# Some rows in the Dataset has frames less than 32\n# Function finds such rows and removes them\ndef remove_bad_rows(dataset):\n    bad_rows = []\n    for i,row in enumerate(dataset):\n     if torch.tensor(row['pixel_values']).shape[1] < 32:\n         bad_rows.append(i)\n\n    # Creating a list of indices excluding the rows to be removed\n    indices_to_keep = [i for i in range(len(dataset)) if i not in bad_rows]\n    \n    # Select only the rows with those indices\n    dataset = dataset.select(indices_to_keep)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.566755Z","iopub.execute_input":"2024-12-11T02:06:27.567029Z","iopub.status.idle":"2024-12-11T02:06:27.574057Z","shell.execute_reply.started":"2024-12-11T02:06:27.567003Z","shell.execute_reply":"2024-12-11T02:06:27.573307Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def pre_process(path):\n\n    # Preparing the training and eval datasets\n\n    # Converts video files to a list of dictionaries containing keys 'video' and 'labels',\n    # where 'video' contains 32 frames each from every video\n    print(\"Creating list of dictionaries...\\n\")\n    list_of_dictionaries = frames_convert_and_create_dataset_dictionary(video_dir=path)\n\n    # Passing above dictionary through VivitImageProcessor\n    print(\"Passing through VivitImageProcessor...\\n\")\n    dataset = create_vivit_dataset(list_of_dictionaries)\n\n    # Converting above dataset to Hugging Face Dataset for fine-tuning\n    print(\"Converting to Hugging Face Dataset...\\n\")\n    dataset_hf = Dataset.from_list(dataset)\n\n    # Encoding classes to the Dataset\n    print(\"Adding class encoding labels...\\n\")\n    dataset_hf = dataset_hf.class_encode_column(\"labels\")\n\n    # Removing bad rows (rows with less than 32 frames)\n    print(\"\\nFinding and removing bad rows...\\n\")\n    dataset_hf = remove_bad_rows(dataset_hf)\n\n\n    # Squeezing; fine-tuning step will throw an errror without this step\n    print(\"\\nSqueezing pixel_values...\\n\")\n    dataset_hf = dataset_hf.map(lambda x: {'pixel_values': torch.tensor(x['pixel_values']).to(device).squeeze()})\n\n    print(\"\\nSuccess.\\n\")\n\n    return dataset_hf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.575073Z","iopub.execute_input":"2024-12-11T02:06:27.575328Z","iopub.status.idle":"2024-12-11T02:06:27.588303Z","shell.execute_reply.started":"2024-12-11T02:06:27.575304Z","shell.execute_reply":"2024-12-11T02:06:27.587639Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Getting training dataset\ntrain_eval_dataset = pre_process(train_videos_path)\n\n# Splitting to Train and Eval sets\ntrain_eval_dataset = train_eval_dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:06:27.589187Z","iopub.execute_input":"2024-12-11T02:06:27.589535Z","iopub.status.idle":"2024-12-11T02:17:20.464902Z","shell.execute_reply.started":"2024-12-11T02:06:27.589491Z","shell.execute_reply":"2024-12-11T02:17:20.463928Z"}},"outputs":[{"name":"stdout","text":"Creating list of dictionaries...\n\nPassing through VivitImageProcessor...\n\nConverting to Hugging Face Dataset...\n\nAdding class encoding labels...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Stringifying the column:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4330271fec5c468d9c39f491f64bab12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting to class labels:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea8faf240b0b4701bd7e3a7dbc6908dc"}},"metadata":{}},{"name":"stdout","text":"\nFinding and removing bad rows...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8155bf1dbaf148a492a911fb6519a90b"}},"metadata":{}},{"name":"stdout","text":"\nSqueezing pixel_values...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"873fe66b24f945158aac3438b6606dc3"}},"metadata":{}},{"name":"stdout","text":"\nSuccess.\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Saving dataset to disk\ntrain_eval_dataset.save_to_disk(\"./processed_datasets/train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T02:17:20.465943Z","iopub.execute_input":"2024-12-11T02:17:20.466189Z","iopub.status.idle":"2024-12-11T02:17:25.445958Z","shell.execute_reply.started":"2024-12-11T02:17:20.466163Z","shell.execute_reply":"2024-12-11T02:17:25.444793Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/4 shards):   0%|          | 0/88 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711e596aac53423495ce0a503e636c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10d1b17fdbd4c0ab891585dbdfc278d"}},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\n\ntrain_dataset = load_from_disk(\"./processed_datasets/train/train\")\neval_dataset = load_from_disk(\"./processed_datasets/train/test\")","metadata":{"execution":{"iopub.status.busy":"2024-12-11T03:21:30.814242Z","iopub.execute_input":"2024-12-11T03:21:30.814912Z","iopub.status.idle":"2024-12-11T03:21:30.829518Z","shell.execute_reply.started":"2024-12-11T03:21:30.814875Z","shell.execute_reply":"2024-12-11T03:21:30.828582Z"},"trusted":true},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"## Defining training arguments","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, EarlyStoppingCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:21:30.831261Z","iopub.execute_input":"2024-12-11T03:21:30.831618Z","iopub.status.idle":"2024-12-11T03:21:30.837026Z","shell.execute_reply.started":"2024-12-11T03:21:30.83158Z","shell.execute_reply":"2024-12-11T03:21:30.836057Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# Defining training arguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",  # Directory to save model checkpoints\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of every epoch\n    save_strategy=\"epoch\",        # Save checkpoints at every epoch\n    logging_dir=\"./logs\",         # Directory to save logs\n    logging_strategy=\"epoch\",\n    save_total_limit=2,           # Only keep the 2 most recent checkpoints\n    load_best_model_at_end=True,  # Automatically load the best model\n    metric_for_best_model=\"eval_loss\",  # Use validation loss to determine the best model\n    greater_is_better=False,      # Lower validation loss is better\n    num_train_epochs=5,          # Set max epochs (early stopping will halt earlier if needed)\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    seed=42,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:21:30.838703Z","iopub.execute_input":"2024-12-11T03:21:30.839029Z","iopub.status.idle":"2024-12-11T03:21:30.882858Z","shell.execute_reply.started":"2024-12-11T03:21:30.83899Z","shell.execute_reply":"2024-12-11T03:21:30.881904Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"## Defining the trainer","metadata":{}},{"cell_type":"code","source":"# Defining the trainer\n\nimport torch\nfrom sklearn.metrics import accuracy_score\nfrom transformers import VivitImageProcessor, VivitForVideoClassification\n\n# Initializing model\nmodel = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n# Initializing image processor\nimage_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n\n\ndef compute_metrics(pred):\n    logits, labels = pred\n    logits = torch.tensor(logits)  # Convert logits to a PyTorch tensor\n    predictions = torch.argmax(logits, dim=-1)\n    acc = accuracy_score(labels, predictions)\n    return {\"accuracy\": acc}\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,                   # Your Hugging Face model\n    args=training_args,            # Training arguments\n    train_dataset=train_dataset,   # Training data\n    eval_dataset=eval_dataset,     # Validation data\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Add early stopping\n    processing_class=image_processor,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:21:30.884422Z","iopub.execute_input":"2024-12-11T03:21:30.885169Z","iopub.status.idle":"2024-12-11T03:21:31.648551Z","shell.execute_reply.started":"2024-12-11T03:21:30.88513Z","shell.execute_reply":"2024-12-11T03:21:31.64748Z"}},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":"##### **[Get your wandb api key](https://wandb.ai/authorize); if you do not already have one. You'll need it to track your training run.**","metadata":{}},{"cell_type":"code","source":"# Loading wandb api key\n\nimport wandb\nimport getpass\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\ntry:\n    my_secret = user_secrets.get_secret(\"wandb_api_key\") \n    wandb.login(key=my_secret)\nexcept:\n    my_secret = getpass.getpass(\"Enter your wandb API Key\")\n    wandb.login(key=my_secret)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:21:31.649785Z","iopub.execute_input":"2024-12-11T03:21:31.650143Z","iopub.status.idle":"2024-12-11T03:21:31.805868Z","shell.execute_reply.started":"2024-12-11T03:21:31.650098Z","shell.execute_reply":"2024-12-11T03:21:31.805008Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"}],"execution_count":77},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:21:31.807589Z","iopub.execute_input":"2024-12-11T03:21:31.807855Z","iopub.status.idle":"2024-12-11T03:42:21.145026Z","shell.execute_reply.started":"2024-12-11T03:21:31.807828Z","shell.execute_reply":"2024-12-11T03:42:21.144155Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='176' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [176/220 20:36 < 05:12, 0.14 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.699400</td>\n      <td>3.525926</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.995700</td>\n      <td>0.071005</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.343600</td>\n      <td>1.156387</td>\n      <td>0.800000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.316900</td>\n      <td>2.825284</td>\n      <td>0.700000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=176, training_loss=0.8388915116136725, metrics={'train_runtime': 1248.4622, 'train_samples_per_second': 0.352, 'train_steps_per_second': 0.176, 'total_flos': 9.04954336247808e+17, 'train_loss': 0.8388915116136725, 'epoch': 4.0})"},"metadata":{}}],"execution_count":78},{"cell_type":"markdown","source":"## Saving the model","metadata":{}},{"cell_type":"code","source":"# Saving the model\ntrainer.save_model(\"./vivit_finetuned_deception_detection\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:42:21.146191Z","iopub.execute_input":"2024-12-11T03:42:21.146505Z","iopub.status.idle":"2024-12-11T03:42:21.986173Z","shell.execute_reply.started":"2024-12-11T03:42:21.146478Z","shell.execute_reply":"2024-12-11T03:42:21.98517Z"}},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"markdown","source":"## Creating the test dataset","metadata":{}},{"cell_type":"code","source":"# Setting test video path\ntest_videos_path = \"/kaggle/input/real-life-deception-detection-dataset/Real-life Deception Detection Dataset With Train Test/Test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:42:21.987347Z","iopub.execute_input":"2024-12-11T03:42:21.987634Z","iopub.status.idle":"2024-12-11T03:42:21.992202Z","shell.execute_reply.started":"2024-12-11T03:42:21.987607Z","shell.execute_reply":"2024-12-11T03:42:21.991308Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"from transformers import VivitImageProcessor\nimport av\n\ndef create_test_data(path):\n    \n    image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n    \n    test_dataset=[]\n    test_labels = []\n\n    # Creating list of all video file names\n    video_files = [\n            os.path.join(path, f)\n            for f in os.listdir(path)\n            if f.endswith(\".mp4\")\n    ] \n\n    for file in video_files:\n\n        if \"lie\" in file.lower():\n            label = 0\n        elif \"truth\" in file.lower():\n            label = 1\n            \n        container = av.open(file)\n\n        # sample 32 frames\n        indices = sample_frame_indices(no_of_frames=NO_OF_FRAMES, frame_sample_rate=FRAME_SAMPLE_RATE, seg_len=container.streams.video[0].frames)\n        \n        video = read_video_pyav(container=container, indices=indices)\n        \n        image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n        \n        inputs = image_processor(list(video), return_tensors=\"pt\")\n\n        test_dataset.append(inputs)\n        test_labels.append(label)\n    \n    return test_dataset, test_labels    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:42:21.993289Z","iopub.execute_input":"2024-12-11T03:42:21.993563Z","iopub.status.idle":"2024-12-11T03:42:22.003716Z","shell.execute_reply.started":"2024-12-11T03:42:21.993538Z","shell.execute_reply":"2024-12-11T03:42:22.002963Z"}},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":"## Predicting for test data","metadata":{}},{"cell_type":"code","source":"test_data, test_labels = create_test_data(test_videos_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:42:22.004604Z","iopub.execute_input":"2024-12-11T03:42:22.005339Z","iopub.status.idle":"2024-12-11T03:42:26.561441Z","shell.execute_reply.started":"2024-12-11T03:42:22.005294Z","shell.execute_reply":"2024-12-11T03:42:26.560717Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"from transformers import VivitModel\n\ndef test(test_data):\n    \n    model = VivitForVideoClassification.from_pretrained(\"./vivit_finetuned_deception_detection\")\n\n    predictions = []\n    \n    for inputs in test_data:\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            logits = outputs.logits\n        \n        # model predicts one of the 400 Kinetics-400 classes\n        predicted_label = logits.argmax(-1).item()\n\n        predictions.append(predicted_label)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:42:26.562434Z","iopub.execute_input":"2024-12-11T03:42:26.562707Z","iopub.status.idle":"2024-12-11T03:42:26.568653Z","shell.execute_reply.started":"2024-12-11T03:42:26.562681Z","shell.execute_reply":"2024-12-11T03:42:26.567556Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"pred_labels = test(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:42:26.571309Z","iopub.execute_input":"2024-12-11T03:42:26.571648Z","iopub.status.idle":"2024-12-11T03:43:31.486131Z","shell.execute_reply.started":"2024-12-11T03:42:26.571621Z","shell.execute_reply":"2024-12-11T03:43:31.48528Z"}},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":"## Accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n\n\nscore = accuracy_score(y_true = test_labels, y_pred=pred_labels)\n\nscore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T03:43:31.487687Z","iopub.execute_input":"2024-12-11T03:43:31.488016Z","iopub.status.idle":"2024-12-11T03:43:31.49619Z","shell.execute_reply.started":"2024-12-11T03:43:31.487981Z","shell.execute_reply":"2024-12-11T03:43:31.495336Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"0.25"},"metadata":{}}],"execution_count":85},{"cell_type":"markdown","source":"#### Accuracy can be improved by training with more frames per video, by increasing 'NO_OF_FRAMES' and/or 'FRAME_SAMPLE_RATE' variables, given better hardware capabilities.","metadata":{}}]}